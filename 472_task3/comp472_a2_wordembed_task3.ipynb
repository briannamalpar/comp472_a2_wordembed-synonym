{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58408f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (23.3.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (69.0.2)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.26.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.11.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: fasttext in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fasttext) (2.11.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fasttext) (69.0.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fasttext) (1.26.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools\n",
    "!pip install --no-cache-dir gensim\n",
    "!pip install numpy scipy\n",
    "\n",
    "!pip install nltk\n",
    "!pip install fasttext\n",
    "\n",
    "!pip install requests\n",
    "!pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2010c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explicitly set the path to the SSL certificate file in your Python script or notebook.\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00e5d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32425bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/briannam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import random\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt') \n",
    "\n",
    "#helper functions\n",
    "def read_book_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading content from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to preprocess text and tokenize into sentences(list of sentences)\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    return tokenized_sentences\n",
    "\n",
    "def read_synonym_data(file_path):\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "\n",
    "def find_best_synonym(word, answer_options, model, topn=1):\n",
    "    try:\n",
    "        # calculate the cosine similarity between the word and each answer option\n",
    "        similarities = [(option, model.wv.similarity(word, option.lower())) for option in answer_options]\n",
    "\n",
    "        sorted_options = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # return highest similarity\n",
    "        best_guess = sorted_options[0][0]\n",
    "        return best_guess\n",
    "\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "def generate_label(question_word, correct_answer, model_guess, model):\n",
    "    if (\n",
    "        correct_answer not in [question_word] + [model_guess]\n",
    "        and ((model_guess not in model.wv.key_to_index or question_word not in model.wv.key_to_index))\n",
    "    ):\n",
    "        return \"guess\"\n",
    "    elif model_guess == correct_answer:\n",
    "        return \"correct\"\n",
    "    else:\n",
    "        return \"wrong\"\n",
    "\n",
    "\n",
    "def process_synonym_test_data(data, model):\n",
    "    correct_count = 0\n",
    "    valid_count = 0\n",
    "    results = []\n",
    "\n",
    "    for entry in data:\n",
    "        question_word = entry['question']\n",
    "        correct_answer = entry['answer']\n",
    "        guess_options = [entry[str(i)] for i in range(4)]  # Options are in columns 0 to 3\n",
    "\n",
    "        model_guess = find_best_synonym(question_word, guess_options, model)\n",
    "        \n",
    "        # if correct answer not in model, randomly select one as system guess\n",
    "        if correct_answer not in [question_word] + [model_guess]:\n",
    "            model_guess = random.choice(guess_options)\n",
    "\n",
    "        # generate label\n",
    "        label = generate_label(question_word, correct_answer, model_guess, model)\n",
    "\n",
    "        if label == 'correct':\n",
    "            correct_count += 1\n",
    "        if label != 'guess':\n",
    "            valid_count += 1\n",
    "\n",
    "        results.append({\n",
    "            'question_word': question_word,\n",
    "            'correct_answer': correct_answer,\n",
    "            'model_guess': model_guess,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "    return results, correct_count, valid_count\n",
    "    \n",
    "def write_to_csv(results, file_name):\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        csv_info = ['question_word', 'correct_answer', 'model_guess', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_info)\n",
    "        writer.writeheader()\n",
    "        for result in results:\n",
    "            writer.writerow(result)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24346431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synonym_test_file = '/Users/briannam/Downloads/A2-DataSet/synonym.csv'\n",
    "#home computer: 'Users/briannam/Downloads/A2-Dataset/synonym.csv'\n",
    "#uni computer: 'C:/Users/b_malpar/Downloads/A2-DataSet/synonym.csv'\n",
    "synonym_test_data = read_synonym_data(synonym_test_file)\n",
    "\n",
    "books = [\n",
    "    '/Users/briannam/Downloads/pg58866-tmotl.txt', #the murder on the links\n",
    "    '/Users/briannam/Downloads/pg2199-ti.txt', #the illiad\n",
    "    '/Users/briannam/Downloads/pg514-lw.txt', #little women\n",
    "    '/Users/briannam/Downloads/pg161-ss.txt', #sense and sensibility\n",
    "    '/Users/briannam/Downloads/pg1399-ak.txt', #ana karenina\n",
    "    '/Users/briannam/Downloads/pg4078-tpodg.txt' #the picture of dorian gray\n",
    "]\n",
    "\n",
    "all_sentences = []\n",
    "#download and preprocess books\n",
    "for book in books:\n",
    "    book_content = read_book_from_file(book)  # Update this function accordingly\n",
    "    if book_content:\n",
    "        tokenized_sentences = preprocess_text(book_content)\n",
    "        all_sentences.extend(tokenized_sentences)\n",
    "    \n",
    "# Define window sizes and embedding sizes to experiment with\n",
    "window_sizes = [5, 10]  # W5 and W10\n",
    "embedding_sizes = [100, 300]  # E100 and E200\n",
    "\n",
    "analysis_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e62fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec models with different parameter combinations\n",
    "for window_size in window_sizes:\n",
    "    for embedding_size in embedding_sizes:\n",
    "        \n",
    "        our_model = Word2Vec(sentences=all_sentences, vector_size=embedding_size, window=window_size, min_count=1, workers=4)\n",
    "\n",
    "        # Perform synonym test and get results\n",
    "        results, correct_count, valid_count = process_synonym_test_data(synonym_test_data, our_model)\n",
    "        \n",
    "        # Save results to <model name>-details.csv\n",
    "        model_name = f'word2vec_model_w{window_size}_e{embedding_size}'\n",
    "        details_file = f'{model_name}-details.csv'\n",
    "        write_to_csv(results, details_file)\n",
    "\n",
    "        # Append analysis results\n",
    "        analysis_results.append({\n",
    "            'model_name': model_name,\n",
    "            'vocab_size': len(model.wv),\n",
    "            'C': correct_count,\n",
    "            'V': valid_count,\n",
    "            'accuracy': correct_count / valid_count if valid_count > 0 else 0\n",
    "        })\n",
    "\n",
    "with open('analysis_ownmodel.csv', 'w', newline='') as csvfile:\n",
    "    csv_info = ['model_name', 'vocab_size', 'C', 'V', 'accuracy']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_info)\n",
    "    writer.writeheader()\n",
    "    for result in analysis_results:\n",
    "        writer.writerow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c240b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
