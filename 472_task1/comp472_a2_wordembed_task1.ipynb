{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (23.3.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (69.0.2)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.26.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.11.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m379.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m320.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m129.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m204.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m453.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.10.3 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools\n",
    "!pip install --no-cache-dir gensim\n",
    "!pip install numpy scipy\n",
    "\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "\n",
    "#word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin 2', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#helper functions\n",
    "def read_synonym_data(file_path):\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "\n",
    "def find_best_synonym(word, answer_options, model, topn=1):\n",
    "    #get embedding vector for word\n",
    "    try:\n",
    "        # Get the embedding vector for the word\n",
    "        word_vector = model[word]\n",
    "\n",
    "        # Calculate the cosine similarity between the word and each answer option\n",
    "        similarities = [(option, model.similarity(word, option.lower())) for option in answer_options]\n",
    "\n",
    "        # Sort the options based on similarity, in descending order\n",
    "        sorted_options = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the option with the highest similarity\n",
    "        best_guess = sorted_options[0][0]\n",
    "        return best_guess\n",
    "    # Word not in vocabulary\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "def generate_label(question_word, correct_answer, model_guess):\n",
    "    if model_guess is None or correct_answer not in [question_word] + [model_guess]:\n",
    "        return 'guess'\n",
    "    elif model_guess == correct_answer:\n",
    "        return 'correct'\n",
    "    else:\n",
    "        return 'wrong'\n",
    "\n",
    "def process_synonym_test_data(data, model):\n",
    "    correct_count = 0\n",
    "    valid_count = 0\n",
    "    results = []\n",
    "\n",
    "    for entry in data:\n",
    "        question_word = entry['question']\n",
    "        correct_answer = entry['answer']\n",
    "        guess_options = [entry[str(i)] for i in range(4)]  # options are in columns 0 to 3\n",
    "\n",
    "        model_guess = find_best_synonym(question_word, guess_options, model)\n",
    "\n",
    "        # Generate the label\n",
    "        label = generate_label(question_word, correct_answer, model_guess)\n",
    "        \n",
    "        if label == 'correct':\n",
    "            correct_count += 1\n",
    "        if label != 'guess':\n",
    "            valid_count += 1\n",
    "\n",
    "        # Append the result to the results list\n",
    "        results.append({\n",
    "            'question_word': question_word,\n",
    "            'correct_answer': correct_answer,\n",
    "            'model_guess': model_guess,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "    return results, correct_count, valid_count\n",
    "    \n",
    "def write_to_csv(results, file_name):\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        csv_info = ['question_word', 'correct_answer', 'model_guess', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_info)\n",
    "        writer.writeheader()\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_test_file = '/Users/briannam/Downloads/A2-DataSet/synonym.csv'\n",
    "#home computer: 'Users/briannam/Downloads/A2-Dataset/synonym.csv'\n",
    "#uni computer: 'C:/Users/b_malpar/Downloads/A2-DataSet/synonym.csv'\n",
    "synonym_test_data = read_synonym_data(synonym_test_file)\n",
    "\n",
    "results, correct_count, valid_count = process_synonym_test_data(synonym_test_data, word2vec_model)\n",
    "\n",
    "accuracy = correct_count / valid_count if valid_count > 0 else 0\n",
    "    \n",
    "# write results to csv file\n",
    "write_to_csv(results, 'word2vec-google-news-300-details.csv')\n",
    "\n",
    "#write analysis to csv file\n",
    "with open('analysis.csv', 'w', newline='') as csvfile:\n",
    "    csv_info = ['model_name', 'vocab_size', 'C', 'V', 'accuracy']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_info)\n",
    "    writer.writeheader()\n",
    "\n",
    "    model_name = 'word2vec-google-news-300'\n",
    "    vocabulary_size = 3000000\n",
    "\n",
    "    writer.writerow({\n",
    "        'model_name': model_name,\n",
    "        'vocab_size': vocabulary_size,\n",
    "        'C': correct_count,\n",
    "        'V': valid_count,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
